%%% lorem.tex --- 
%% 
%% Filename: lorem.tex
%% Description: 
%% Author: Ola Leifler
%% Maintainer: 
%% Created: Wed Nov 10 09:59:23 2010 (CET)
%% Version: $Id$
%% Version: 
%% Last-Updated: Wed Nov 10 09:59:47 2010 (CET)
%%           By: Ola Leifler
%%     Update #: 2
%% URL: 
%% Keywords: 
%% Compatibility: 
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Commentary: 
%% 
%% 
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Change log:
%% 
%% 
%% RCS $Log$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Code:

\chapter{Diskussion}
\label{cha:discussion}

Nedan diskuteras implementationen, fördelar och nackdelar med Entropi-estimeringen, Huffman-kodningen samt LZW-kodningen.

\section{Entropi-estimering}
Entropin var till en början lite problematisk att implementera. Det uppstod problem när sannolikheterna för par av symboler och tripplar av symboler skulle estimeras. Det första som gjordes var att addera två ASCII-tecken men jag insåg sedan att symbolerna AB kommer mappas till samma symbol BA och detta är förstås inte tanken. Detta löstes sedan genom att konkatenera deras bitsekvenser som förklaras i metodavsnittet. Efter det kunde de betingade entropierna beräknas på ett korrekt och enkelt sätt genom att använda kedjeregeln.

\section{Huffman-kodning}
Huffman-kodningen implementerades smärtfritt. Fördelen med Huffman-kodning är att den alltid generar en optimal kod, detta eftersom vi alltid tar och adderar de två minsta sannolikheterna när vi bygger upp Huffman-trädet. Det gör att de symboler som förekommer ofta i källan kommer att kodas med få bitar. Huffman-kodning kommer heller inte att ha några onödiga extra 0:or framför kodordet som kan förekomma i LZW-koden. Detta leder till en optimal kod, vilket också visas i resultatet då vi kommer väldigt nära den optimala entropin. En annan fördel med Huffman-kodning är att den är enkel att implementera, när Huffman-trädet är uppbyggt är det bara att läsa av symbol för symbol i källan och och slå upp den symbolen i Huffman-trädet, vilket ger den motsvarande bitsekvensen.
Den stora nackdelen med Huffman-kodning är att vi måste spara statistik för hur källan ser ut. I mitt fall har jag sparat hela frekvenstabellen i en så kallad header, detta gör att den komprimerade filen således blir större än vad den egentligen behöver vara. I exemplet som presenterades i resultatavsnittet ser vi att den komprimerade filen blir större än orginalfilen. Detta beror alltså på att vi måste addera frekvenstabellen till den komprimerade filen för att kunna avkomprimera den. Utan frekvenstabellen skulle filen endast kodas med 18 bitar, vilket kan jämföras med LZW-kodningen som gav hela 62 bitar.

\section{LZW-kodning}
LZW-kodningen implementerades utan problem. Enda problemet var att först visste jag inte hur många bitar man skulle koda symbolens index med, så jag begränsade min ordbok till ${2}^{20}$, det vill säga jag skrev varje index med 20 bitar. Blev ordboken full, så slutade jag lägga in fler symboler och körde på de symboler jag hade i ordboken. Denna metod presterade bra ändå tyckte jag, speciellt på stora filer, eftersom vid stora filer fylls ändå ordboken. Vid små filer däremot blev inte resultatet lika tillfredställande, en fil blev större vid komprimering än vad den var innan komprimering till exempel. 
Föreläsningen om LZW-kodning var sedan givande. Jag ändrade till att skriva antalet bitar till att bero på nuvarande storlek på ordboken istället, detta betyder att i bästa fall skriver vi med optimalt antal bitar. Däremot kan vi ibland fortfarande skriva onödiga bitar eftersom om vi får en träff i ordboken med en symbol som motsvarar index 3, som i bitar skrivs 11. Låt oss till exempel säga att storleken på ordboken för tillfället är 1500 då kommer vi skriva med 11 bitar, det vill säga 00000000011, vilket inte är optimalt såklart. Anledningen till att vi måste ta hänsyn till storleken på ordboken är att vi annars inte vet hur många bitar vi ska läsa av när vi avkomprimerar den LZW-kodade filen.
Fördelen med LZW-kodning är att vi inte behöver veta någonting om hur den tidigare källan ser ut. Det vill säga vi behöver till exempel inte veta frekvenserna för varje symbol, vilket vi behöver veta vid Huffman-kodning. Detta leder till att den LZW-kodade filen inte behöver bli extra stor. LZW-kodning kodar i allmänhet annars väldigt bra.
Nackdelen med LZW-kodning är som nämnts ovan att den inte presterar en optimal kod. Vi kan tillexempel inte koda 11 med bara bitarna 11 utan vi måste koda den beroende på hur stor ordboken är för tillfället. LZW-kodning bygger på upprepningar av symboler och är därför inte så användbar om det inte förekommer mycket upprepningar i källan. LZW-kodning är således användbar för stora källor, då finns chans för fler upprepningar i källan. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% lorem.tex ends here

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "demothesis"
%%% End: 
