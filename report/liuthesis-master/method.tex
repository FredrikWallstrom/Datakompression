%%% lorem.tex --- 
%% 
%% Filename: lorem.tex
%% Description: 
%% Author: Ola Leifler
%% Maintainer: 
%% Created: Wed Nov 10 09:59:23 2010 (CET)
%% Version: $Id$
%% Version: 
%% Last-Updated: Wed Nov 10 09:59:47 2010 (CET)
%%           By: Ola Leifler
%%     Update #: 2
%% URL: 
%% Keywords: 
%% Compatibility: 
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Commentary: 
%% 
%% 
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Change log:
%% 
%% 
%% RCS $Log$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Code:

\chapter{Metod}
\label{cha:method}

Nedan gås igenom hur Entropi-estimering, Huffman-kodning och LZW-kodning har genomförts.

\subsection{Entropi-estimering}
För att göra en Entropi-estimering av givna källor, som angavs i laborationens uppgift, skrevs ett program i C++. Programmet utgick ifrån en funktion som beräknade entropin för en given källa enligt formeln som beskrivs i teoriavsnittet. Funktionen tar in en sannolikhetstabell som mappar varje symbol mot dess sannolikhet att den förekommer i källan. Denna sannolikhetstabell behövde alltså genereras innan entropin kunde beräknas.

För att generera en sannolikhetstabell estimerades sannolikheterna för varje symbol i källan. Detta gjordes genom att först spara hela filen i en lista, för att sedan iterera över den och addera 1 i en ny lista för varje påträffad symbol i källan. Denna nya lista mappar således symboler mot hur många gånger den förekommer i källan, det vill säga, en frekvenstabell. Denna frekvenstabell användes sedan för att beräkna sannolikhetstabellen genom att dividera varje frekvens med källans storlek. Den önskade entropin kunde sedan beräknas.

I laborationen har även betingade entropier beräknats för källorna. En betingad entropi betyder att vi beräknar entropi då vi känner till hur källan ser ut K steg tillbaka i tiden. De betingade entropierna som har beräknats är då K = 1 och då K = 2, detta utrycks genom följande, H(Xi|Xi-1) och H(Xi|Xi-1, Xi-2). För att beräkna de betingade entropierna krävdes en sannolikhetstabell för par av symboler och för tripplar av symboler. Dessa sannolikhetstabeller togs fram på motsvarande sätt som för en symbol, med undantag att två respektive tre symboler lästes in i taget. Dessa symboler adderades sedan genom att konkatenera dess bit representation, detta skapar en unik ny symbol. Vi har då sannolikhetstabeller för par och tripplar av symboler. Sedan beräknades den gemensamma entropin, H(X1, X2) och H(X1, X2, X3) med samma entropi funktion som användes för singlar av symboler. Efter det användes kedjeregeln, H(X1, X2, ..., Xn) = H(X1) + H(X2|X1) + ... + H(Xn|X1, ..., Xn-1), för att beräkna den betingade entropin.

\subsection{Huffman-kodning}
För att komprimera godtyckliga filer med Huffman-kodning skrevs ett program i C++. Grundmetoden som användes var den som beskrevs i teoriavsnittet. Först lästes filen in och varje symbol sparades i en lista. Sedan byggdes en frekvenstabell upp på samma vis som gjordes för att estimera entropin. Utifrån denna frekvenstabell byggdes sedan ett Huffman-träd, som beskrivs i teoriavsnittet. Detta Huffman-träd bestod av noder, structar i C++, som innehåller symbolen, frekvensen för symbolen och två stycken pekare till andra noder. För att bygga upp Huffman-trädet används en prioritetskö, det behövs för att noderna i kön behöver vara sortera med den lägsta frekvensen först. Sedan tas de två första noderna ut ur kön och deras frekvens summeras och en ny nod bildas med de två gamla noderna som barn till den nya noden. Den nya noden läggs sedan in i prioritetskön igen. Detta upprepas till bara en nod återstår i prioritetskön, denna nod är rotnoden i Huffman-trädet.

När Huffman-trädet är uppbyggt byggdes sedan en så kallad kodningskarta upp, det vill säga en lista som mappar symboler mot sekvenser av bitar. Detta gjordes genom att rekursivt traversera genom hela Huffman-trädet och varje gång vi gick till höger barn adderades en 1:a och varje gång vi gick till vänster barn adderades en 0:a.

Med denna kodningskarta kodades sedan källan. Detta gjordes genom att återigen öppna upp filen och gå igenom symbol för symbol. För varje symbol i filen adderades motsvarande sekvens av bitar i kodningskartan till en resulterande sträng. Denna sträng skrevs sedan till en ny fil och källan har därmed kodats med Huffman-kodning.

En viktig detalj att nämna här är att den frekvenstabell som genererades i början av Huffman-kodningen sparades i den kodade filen, som en så kallad header. Detta är ett måste eftersom vi behöver veta frekvenserna för varje symbol när vi ska avkomprimera den kodade Huffman-filen.

För att avkomprimera den komprimerade Huffman-filen läses först headern av och frekvenstabellen erhålls. Med denna frekvenstabell byggs sedan samma Huffman-träd upp som vid komprimering av källan. Sedan avkomprimeras Huffman-filen genom att börja från rotnoden och sedan läsa av bit för bit. Vid en 0:a går vi ner i vänstra subträdet och vid en 1:a går vi ner i högra subträdet. När vi stöter på en lövnod adderar vi den motsvarande symbolen till resultatsträngen och börjar om ifrån rotnoden. På så vis byggs originalfilen upp igen.
\subsection{LZW-kodning}
För att komprimera godtyckliga filer med LZW-kodning skrevs ett program i C++. Grundmetoden som användes utgicks ifrån den som beskrevs i teoriavsnittet. Först lästes den givna filen in och varje symbol sparades i en lista. Sedan byggdes ordboken upp, bestående av alla möjliga bytes, 0-255. Sedan itererades det över varje symbol den sparade filen, om den symbolen redan existerade i ordboken så lästes en nästa symbol in och konkateneras till den föregående symbolen. Tillexempel om A läses in och den symbolen redan existerar i ordboken läses nästa symbol B in och konkateneras till A, det ger resultatsymbolen AB. Sedan återupprepas proceduren och det undersöks om symbolen AB finns i ordboken eller inte.

Låt oss nu säga att symbolen AB inte finns i ordboken. Det betyder att vi ska lägga till AB i ordboken och skriva A:s motsvarande index i ordboken till resultatfilen. A:s index skrivs med X antal bitar där X är bas 2 logaritmen av nuvarande storleken på ordboken. Detta upprepas till det inte finns fler symboler kvar att läsa vilket resulterar i en komprimerad LZW-fil.

För att avkomprimera en komprimerad LZW-fil läses först LZW-filen in och varje bit sparades i en lista. Sedan byggdes samma motsvarande ordbok upp som vid komprimering av filen med undantaget att denna gång mappas index mot bytes. Sedan läser vi av X antal bitar ifrån listan där X är bas 2 logaritmen av nuvarande storleken på ordboken. Dessa byter transformeras till motsvarande siffra, med denna siffra slås motsvarande symbol upp i ordboken. Under tiden i avkomprimeringen byggs ordboken upp så att den blir identisk med den ordbok som användes vid komprimeringen av källan. Hur detta görs förklaras i teoriavsnittet. På detta vis erhålls en identisk källa som innan komprimering.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% lorem.tex ends here

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "demothesis"
%%% End: 
